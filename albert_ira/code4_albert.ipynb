{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from gensim.models import KeyedVectors\n",
    "from albert import *\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100  822M  100  822M    0     0  4265k      0  0:03:17  0:03:17 --:--:-- 5021k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /home/salsabila.pranida/ai_project_aes/word_embeddings/glove.6B.zip\n",
      "  inflating: /home/salsabila.pranida/ai_project_aes/word_embeddings/glove.6B.50d.txt  \n",
      "  inflating: /home/salsabila.pranida/ai_project_aes/word_embeddings/glove.6B.100d.txt  \n",
      "  inflating: /home/salsabila.pranida/ai_project_aes/word_embeddings/glove.6B.200d.txt  \n",
      "  inflating: /home/salsabila.pranida/ai_project_aes/word_embeddings/glove.6B.300d.txt  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  279M  100  279M    0     0  99.7M      0  0:00:02  0:00:02 --:--:-- 99.7M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p ~/ai_project_aes/word_embeddings\n",
    "curl -o ~/ai_project_aes/word_embeddings/glove.6B.zip https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "unzip ~/ai_project_aes/word_embeddings/glove.6B.zip -d ~/ai_project_aes/word_embeddings\n",
    "# rm ~/ai_project_aes/word_embeddings/glove.6B.zip  \n",
    "\n",
    "curl -o ~/ai_project_aes/word_embeddings/wiki.en.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_DIR = '../'  # Navigate one level up to access directories outside of albert_ira\n",
    "DATASET_DIR = os.path.join(BASE_DIR, 'dataset')\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'result')\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "GLOVE_PATH = os.path.join(BASE_DIR, 'word_embeddings/glove.6B.300d.txt')\n",
    "FASTTEXT_PATH = os.path.join(BASE_DIR, 'word_embeddings/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "glove_model = load_glove_model(GLOVE_PATH)\n",
    "fasttext_model = load_fasttext_model(FASTTEXT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "df_label = df['domain1_score']\n",
    "df = df.dropna(axis=1)\n",
    "df = df.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\n",
    "maximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])\n",
    "\n",
    "old_min = minimum_scores[df['essay_set']]\n",
    "old_max = maximum_scores[df['essay_set']]\n",
    "old_range = old_max - old_min\n",
    "new_min = 0\n",
    "new_max = 100\n",
    "new_range = (new_max - new_min)  \n",
    "df['score'] = (((df['domain1_score'] - old_min) * new_range) / old_range) + new_min\n",
    "\n",
    "# round score to nearest integer for cohen kappa calculation\n",
    "df_label = np.round(df['score']).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  score  \n",
       "0              8   60.0  \n",
       "1              9   70.0  \n",
       "2              7   50.0  \n",
       "3             10   80.0  \n",
       "4              8   60.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12976"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with embedding type: ALBERT only\n",
      "\n",
      "Embedding: ALBERT only | Epoch 1/10, Loss: 420.58360479204975\n",
      "Embedding: ALBERT only | Epoch 2/10, Loss: 357.06914735097547\n",
      "Embedding: ALBERT only | Epoch 3/10, Loss: 334.4607959579797\n",
      "Embedding: ALBERT only | Epoch 4/10, Loss: 341.6871045616632\n",
      "Embedding: ALBERT only | Epoch 5/10, Loss: 332.7350115122156\n",
      "Embedding: ALBERT only | Epoch 6/10, Loss: 325.9642953174691\n",
      "Embedding: ALBERT only | Epoch 7/10, Loss: 322.0063424345525\n",
      "Embedding: ALBERT only | Epoch 8/10, Loss: 323.08384682841955\n",
      "Embedding: ALBERT only | Epoch 9/10, Loss: 316.53450622617373\n",
      "Embedding: ALBERT only | Epoch 10/10, Loss: 313.50815373613216\n",
      "Model and embedding size saved to ../result\n",
      "\n",
      "Training with embedding type: glove\n",
      "\n",
      "Embedding: glove | Epoch 1/10, Loss: 440.60937239613116\n",
      "Embedding: glove | Epoch 2/10, Loss: 361.41251565348387\n",
      "Embedding: glove | Epoch 3/10, Loss: 355.11866864878886\n",
      "Embedding: glove | Epoch 4/10, Loss: 349.80233386705396\n",
      "Embedding: glove | Epoch 5/10, Loss: 326.7464471708277\n",
      "Embedding: glove | Epoch 6/10, Loss: 334.15353524923694\n",
      "Embedding: glove | Epoch 7/10, Loss: 324.05788350215494\n",
      "Embedding: glove | Epoch 8/10, Loss: 327.03438072616035\n",
      "Embedding: glove | Epoch 9/10, Loss: 324.4258364984545\n",
      "Embedding: glove | Epoch 10/10, Loss: 319.26894663148005\n",
      "Model and embedding size saved to ../result\n",
      "\n",
      "Training with embedding type: fasttext\n",
      "\n",
      "Embedding: fasttext | Epoch 1/10, Loss: 426.10407841921955\n",
      "Embedding: fasttext | Epoch 2/10, Loss: 352.50638944158567\n",
      "Embedding: fasttext | Epoch 3/10, Loss: 335.5864240267244\n",
      "Embedding: fasttext | Epoch 4/10, Loss: 328.29929284652684\n",
      "Embedding: fasttext | Epoch 5/10, Loss: 327.97087697439093\n",
      "Embedding: fasttext | Epoch 6/10, Loss: 325.1484262880816\n",
      "Embedding: fasttext | Epoch 7/10, Loss: 317.76926266063344\n",
      "Embedding: fasttext | Epoch 8/10, Loss: 316.78893213022286\n",
      "Embedding: fasttext | Epoch 9/10, Loss: 311.6496042595439\n",
      "Embedding: fasttext | Epoch 10/10, Loss: 320.57384357393613\n",
      "Model and embedding size saved to ../result\n"
     ]
    }
   ],
   "source": [
    "# Train, save, and evaluate models for each embedding type\n",
    "embedding_types = [None, \"glove\", \"fasttext\"]\n",
    "for embedding_type in embedding_types:\n",
    "    # Prepare embeddings\n",
    "    embeddings_and_sizes = df['essay'].apply(lambda x: create_combined_embedding(x, embedding_type, glove_model, fasttext_model))\n",
    "    df['embeddings'], embedding_sizes = zip(*embeddings_and_sizes)\n",
    "    \n",
    "    # Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        np.stack(df['embeddings'].values), df_label.values, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Define and train the model with the correct input size\n",
    "    input_shape = X_train_tensor.shape[1]\n",
    "    model = RegressionModel(input_shape).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 8\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f\"\\nTraining with embedding type: {embedding_type or 'ALBERT only'}\\n\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Embedding: {embedding_type or 'ALBERT only'} | Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "    model_filename = f\"regression_model_{embedding_type or 'albert'}.pth\"\n",
    "    embedding_size_filename = f\"embedding_size_{embedding_type or 'albert'}.npy\"\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, model_filename))\n",
    "    np.save(os.path.join(SAVE_DIR, embedding_size_filename), input_shape)\n",
    "    print(f\"Model and embedding size saved to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "ALBERT only:\n",
      "  Accuracy: 0.026964560862865947\n",
      "  F1 Score: 0.04364821799673594\n",
      "  Quadratic Kappa Score: 0.7792425291827828\n",
      "ALBERT + GloVe:\n",
      "  Accuracy: 0.02465331278890601\n",
      "  F1 Score: 0.03868426421516743\n",
      "  Quadratic Kappa Score: 0.746800779856192\n",
      "ALBERT + FastText:\n",
      "  Accuracy: 0.025423728813559324\n",
      "  F1 Score: 0.03865297557945341\n",
      "  Quadratic Kappa Score: 0.748236734329295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm-salsabila.pranida-49173/ipykernel_772877/875738856.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "/tmp/slurm-salsabila.pranida-49173/ipykernel_772877/875738856.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "/tmp/slurm-salsabila.pranida-49173/ipykernel_772877/875738856.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "# Evaluation results dictionary\n",
    "results = {}\n",
    "for embedding_type in embedding_types:\n",
    "    # Set model paths and load embedding size based on embedding type\n",
    "    if embedding_type is None:\n",
    "        model_filename = \"regression_model_albert.pth\"\n",
    "        embedding_size_filename = \"embedding_size_albert.npy\"\n",
    "        embedding_type_name = \"ALBERT only\"\n",
    "    elif embedding_type == \"glove\":\n",
    "        model_filename = \"regression_model_glove.pth\"\n",
    "        embedding_size_filename = \"embedding_size_glove.npy\"\n",
    "        embedding_type_name = \"ALBERT + GloVe\"\n",
    "    elif embedding_type == \"fasttext\":\n",
    "        model_filename = \"regression_model_fasttext.pth\"\n",
    "        embedding_size_filename = \"embedding_size_fasttext.npy\"\n",
    "        embedding_type_name = \"ALBERT + FastText\"\n",
    "    \n",
    "    # Load the model with the correct input size\n",
    "    embedding_size_path = os.path.join(SAVE_DIR, embedding_size_filename)\n",
    "    input_size = int(np.load(embedding_size_path))  # Load the saved embedding size\n",
    "\n",
    "    model = RegressionModel(input_size).to(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    model_path = os.path.join(SAVE_DIR, model_filename)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Resize and transfer X_test_tensor to match model input size\n",
    "    X_test_tensor_resized = X_test_tensor[:, :input_size].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor_resized).cpu().numpy().flatten()  # Flatten for comparison\n",
    "        y_pred_rounded = np.round(y_pred)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_rounded)\n",
    "    f1 = f1_score(y_test, y_pred_rounded, average='weighted')\n",
    "    kappa_score = cohen_kappa_score(y_test, y_pred_rounded, weights='quadratic')\n",
    "    \n",
    "    # Store the results\n",
    "    results[embedding_type_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'kappa_score': kappa_score\n",
    "    }\n",
    "\n",
    "# Print final evaluation results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for embedding_type_name, metrics in results.items():\n",
    "    print(f\"{embedding_type_name}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']}\")\n",
    "    print(f\"  F1 Score: {metrics['f1_score']}\")\n",
    "    print(f\"  Quadratic Kappa Score: {metrics['kappa_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testContent(content, embedding_type=None):\n",
    "    \"\"\"Generate an essay score for a given content using the trained model.\"\"\"\n",
    "    \n",
    "    # Generate the combined embedding and get the actual size\n",
    "    \n",
    "    embedding, actual_embedding_size = create_combined_embedding(\n",
    "        content, embedding_type=embedding_type, \n",
    "        glove_model=glove_model if embedding_type == \"glove\" else None,\n",
    "        fasttext_model=fasttext_model if embedding_type == \"fasttext\" else None\n",
    "    )\n",
    "\n",
    "    embedding = torch.tensor(embedding, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "    # Load the expected embedding size and model path based on embedding type\n",
    "    embedding_size_filename = f\"embedding_size_{embedding_type or 'albert'}.npy\"\n",
    "    model_filename = f\"regression_model_{embedding_type or 'albert'}.pth\"\n",
    "    \n",
    "    # Load the saved embedding size to ensure correct model initialization\n",
    "    embedding_size_path = os.path.join(SAVE_DIR, embedding_size_filename)\n",
    "    expected_embedding_size = int(np.load(embedding_size_path))\n",
    "    \n",
    "    # Initialize the model with the correct input size\n",
    "    model = RegressionModel(expected_embedding_size).to(device)\n",
    "    model_path = os.path.join(SAVE_DIR, model_filename)\n",
    "    model = load_model(model_path, expected_embedding_size)\n",
    "    model.eval()\n",
    "\n",
    "    # If necessary, resize the embedding to match the model's expected input size\n",
    "    embedding_resized = embedding[:, :expected_embedding_size]\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        pred_score = model(embedding_resized).cpu().numpy()\n",
    "    \n",
    "    # Round the prediction and ensure it's non-negative\n",
    "    pred_score = np.maximum(np.round(pred_score), 0)  # Ensure score is non-negative\n",
    "    return pred_score  # Ensure score is non-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentBad = \"\"\"\n",
    "    In “Let there be dark,” Paul Bogard talks about the importance of darkness.\n",
    "\n",
    "Darkness is essential to humans. Bogard states, “Our bodies need darkness to produce the hormone melatonin, which keeps certain cancers from developing, and our bodies need darkness for sleep, sleep. Sleep disorders have been linked to diabetes, obesity, cardiovascular disease and depression and recent research suggests are main cause of “short sleep” is “long light.” Whether we work at night or simply take our tablets, notebooks and smartphones to bed, there isn’t a place for this much artificial light in our lives.” (Bogard 2). Here, Bogard talks about the importance of darkness to humans. Humans need darkness to sleep in order to be healthy.\n",
    "\n",
    "Animals also need darkness. Bogard states, “The rest of the world depends on darkness as well, including nocturnal and crepuscular species of birds, insects, mammals, fish and reptiles. Some examples are well known—the 400 species of birds that migrate at night in North America, the sea turtles that come ashore to lay their eggs—and some are not, such as the bats that save American farmers billions in pest control and the moths that pollinate 80% of the world’s flora. Ecological light pollution is like the bulldozer of the night, wrecking habitat and disrupting ecosystems several billion years in the making. Simply put, without darkness, Earth’s ecology would collapse...” (Bogard 2). Here Bogard explains that animals, too, need darkness to survive.\n",
    "\"\"\" \n",
    "\n",
    "contentGood = \"\"\"\n",
    "    In response to our world’s growing reliance on artificial light, writer Paul Bogard argues that natural darkness should be preserved in his article “Let There be dark”. He effectively builds his argument by using a personal anecdote, allusions to art and history, and rhetorical questions.\n",
    "\n",
    "Bogard starts his article off by recounting a personal story – a summer spent on a Minnesota lake where there was “woods so dark that [his] hands disappeared before [his] eyes.” In telling this brief anecdote, Bogard challenges the audience to remember a time where they could fully amass themselves in natural darkness void of artificial light. By drawing in his readers with a personal encounter about night darkness, the author means to establish the potential for beauty, glamour, and awe-inspiring mystery that genuine darkness can possess. He builds his argument for the preservation of natural darkness by reminiscing for his readers a first-hand encounter that proves the “irreplaceable value of darkness.” This anecdote provides a baseline of sorts for readers to find credence with the author’s claims.\n",
    "\n",
    "Bogard’s argument is also furthered by his use of allusion to art – Van Gogh’s “Starry Night” – and modern history – Paris’ reputation as “The City of Light”. By first referencing “Starry Night”, a painting generally considered to be undoubtedly beautiful, Bogard establishes that the natural magnificence of stars in a dark sky is definite. A world absent of excess artificial light could potentially hold the key to a grand, glorious night sky like Van Gogh’s according to the writer. This urges the readers to weigh the disadvantages of our world consumed by unnatural, vapid lighting. Furthermore, Bogard’s alludes to Paris as “the famed ‘city of light’”. He then goes on to state how Paris has taken steps to exercise more sustainable lighting practices. By doing this, Bogard creates a dichotomy between Paris’ traditionally alluded-to name and the reality of what Paris is becoming – no longer “the city of light”, but moreso “the city of light…before 2 AM”. This furthers his line of argumentation because it shows how steps can be and are being taken to preserve natural darkness. It shows that even a city that is literally famous for being constantly lit can practically address light pollution in a manner that preserves the beauty of both the city itself and the universe as a whole.\n",
    "\n",
    "Finally, Bogard makes subtle yet efficient use of rhetorical questioning to persuade his audience that natural darkness preservation is essential. He asks the readers to consider “what the vision of the night sky might inspire in each of us, in our children or grandchildren?” in a way that brutally plays to each of our emotions. By asking this question, Bogard draws out heartfelt ponderance from his readers about the affecting power of an untainted night sky. This rhetorical question tugs at the readers’ heartstrings; while the reader may have seen an unobscured night skyline before, the possibility that their child or grandchild will never get the chance sways them to see as Bogard sees. This strategy is definitively an appeal to pathos, forcing the audience to directly face an emotionally-charged inquiry that will surely spur some kind of response. By doing this, Bogard develops his argument, adding gutthral power to the idea that the issue of maintaining natural darkness is relevant and multifaceted.\n",
    "\n",
    "Writing as a reaction to his disappointment that artificial light has largely permeated the prescence of natural darkness, Paul Bogard argues that we must preserve true, unaffected darkness. He builds this claim by making use of a personal anecdote, allusions, and rhetorical questioning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Essay Scores for ALBERT only:\n",
      "  Bad Essay Score: [[91.]]\n",
      "  Good Essay Score: [[92.]]\n",
      "Sample Essay Scores for ALBERT + GloVe:\n",
      "  Bad Essay Score: [[85.]]\n",
      "  Good Essay Score: [[86.]]\n",
      "Sample Essay Scores for ALBERT + FastText:\n",
      "  Bad Essay Score: [[87.]]\n",
      "  Good Essay Score: [[86.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salsabila.pranida/ai_project_aes/albert_ira/albert.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "embedding_types = [None, \"glove\", \"fasttext\"]\n",
    "for embedding_type in embedding_types:\n",
    "    if embedding_type is None:\n",
    "        embedding_type_name = \"ALBERT only\"\n",
    "    elif embedding_type == \"glove\":\n",
    "        embedding_type_name = \"ALBERT + GloVe\"\n",
    "    elif embedding_type == \"fasttext\":\n",
    "        embedding_type_name = \"ALBERT + FastText\"\n",
    "\n",
    "    bad_score = testContent(contentBad, embedding_type=embedding_type)\n",
    "    good_score = testContent(contentGood, embedding_type=embedding_type)\n",
    "    print(f\"Sample Essay Scores for {embedding_type_name}:\")\n",
    "    print(f\"  Bad Essay Score: {bad_score}\")\n",
    "    print(f\"  Good Essay Score: {good_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_aes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
