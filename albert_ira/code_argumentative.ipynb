{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salsabila.pranida/.conda/envs/ai_aes/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA configuration for better debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 11:33:00.820797: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-20 11:33:04.071447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732087985.794805 2213438 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732087986.225924 2213438 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 11:33:10.426879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../\n",
      "Directory already exists: ../dataset\n",
      "Directory already exists: ../result\n",
      "Directory already exists: ../word_embeddings\n",
      "Directory already exists: ../word_embeddings\n"
     ]
    }
   ],
   "source": [
    "# Define Constants\n",
    "BASE_DIR = '../'  # Root directory for accessing files\n",
    "DATASET_DIR = os.path.join(BASE_DIR, 'dataset')\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'result')\n",
    "MODEL_NAME = \"albert-base-v2\"  # ALBERT model identifier\n",
    "GLOVE_PATH = os.path.join(BASE_DIR, 'word_embeddings/glove.6B.300d.txt')\n",
    "FASTTEXT_PATH = os.path.join(BASE_DIR, 'word_embeddings/wiki.en.vec')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "# Load Tokenizer and ALBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "albert_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Ensure directories exist\n",
    "directories = [BASE_DIR, DATASET_DIR, SAVE_DIR, os.path.dirname(GLOVE_PATH), os.path.dirname(FASTTEXT_PATH)]\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory created: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file_path):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings into a dictionary.\n",
    "    :param glove_file_path: Path to the GloVe embedding file.\n",
    "    :return: Dictionary with word-to-vector mappings.\n",
    "    \"\"\"\n",
    "    embedding_dict = {}\n",
    "    with open(glove_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor(np.asarray(values[1:], dtype='float32'))\n",
    "            embedding_dict[word] = vector.to(device)\n",
    "    return embedding_dict\n",
    "\n",
    "def load_fasttext_model(fasttext_file_path):\n",
    "    \"\"\"\n",
    "    Load FastText embeddings into a dictionary.\n",
    "    :param fasttext_file_path: Path to the FastText embedding file.\n",
    "    :return: Dictionary with word-to-vector mappings.\n",
    "    \"\"\"\n",
    "    model = KeyedVectors.load_word2vec_format(fasttext_file_path, binary=False)\n",
    "    return {word: torch.tensor(model[word]).to(device) for word in model.index_to_key}\n",
    "\n",
    "# Load embeddings\n",
    "glove_model = load_glove_model(GLOVE_PATH)\n",
    "fasttext_model = load_fasttext_model(FASTTEXT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>essay_type</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>content</th>\n",
       "      <th>organization</th>\n",
       "      <th>word_choice</th>\n",
       "      <th>sentence_fluency</th>\n",
       "      <th>conventions</th>\n",
       "      <th>language</th>\n",
       "      <th>prompt_adherence</th>\n",
       "      <th>narrativity</th>\n",
       "      <th>style</th>\n",
       "      <th>voice</th>\n",
       "      <th>normalized_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>argumentative</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>argumentative</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "\n",
       "      essay_type  domain1_score  content  organization  word_choice  \\\n",
       "0  argumentative            8.0      4.0           3.0          3.0   \n",
       "1  argumentative            9.0      4.0           4.0          4.0   \n",
       "\n",
       "   sentence_fluency  conventions  language  prompt_adherence  narrativity  \\\n",
       "0               3.0          3.0       0.0               0.0          0.0   \n",
       "1               3.0          4.0       0.0               0.0          0.0   \n",
       "\n",
       "   style  voice  normalized_score  \n",
       "0    0.0    0.0              60.0  \n",
       "1    0.0    0.0              70.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv('processed_essay_dataset.csv', sep=',', encoding='ISO-8859-1')\n",
    "df = df.dropna(subset=['organization', 'word_choice', 'sentence_fluency', 'conventions'])  # Ensure all required columns are present\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4296"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Organization': {3.0: 1198,\n",
       "  4.0: 1068,\n",
       "  2.0: 578,\n",
       "  5.0: 548,\n",
       "  8.0: 221,\n",
       "  6.0: 171,\n",
       "  1.0: 154,\n",
       "  7.0: 135,\n",
       "  9.0: 87,\n",
       "  12.0: 69,\n",
       "  10.0: 29,\n",
       "  11.0: 19,\n",
       "  13.0: 11,\n",
       "  14.0: 3,\n",
       "  15.0: 3,\n",
       "  16.0: 2},\n",
       " 'Word Choice': {3.0: 1283,\n",
       "  4.0: 1069,\n",
       "  2.0: 533,\n",
       "  5.0: 479,\n",
       "  8.0: 289,\n",
       "  1.0: 145,\n",
       "  6.0: 141,\n",
       "  7.0: 127,\n",
       "  9.0: 79,\n",
       "  12.0: 73,\n",
       "  10.0: 39,\n",
       "  11.0: 15,\n",
       "  13.0: 12,\n",
       "  15.0: 7,\n",
       "  14.0: 4,\n",
       "  16.0: 1},\n",
       " 'Sentence Fluency': {4.0: 1253,\n",
       "  3.0: 1224,\n",
       "  5.0: 574,\n",
       "  2.0: 371,\n",
       "  8.0: 222,\n",
       "  6.0: 170,\n",
       "  7.0: 149,\n",
       "  1.0: 112,\n",
       "  9.0: 85,\n",
       "  12.0: 74,\n",
       "  10.0: 28,\n",
       "  11.0: 18,\n",
       "  14.0: 6,\n",
       "  13.0: 6,\n",
       "  15.0: 4},\n",
       " 'Conventions': {4.0: 1195,\n",
       "  3.0: 1190,\n",
       "  5.0: 529,\n",
       "  2.0: 500,\n",
       "  6.0: 235,\n",
       "  8.0: 182,\n",
       "  1.0: 143,\n",
       "  7.0: 131,\n",
       "  9.0: 71,\n",
       "  10.0: 37,\n",
       "  12.0: 36,\n",
       "  11.0: 34,\n",
       "  13.0: 8,\n",
       "  14.0: 4,\n",
       "  15.0: 1}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique values for each feature\n",
    "organization_counts = df['organization'].value_counts()\n",
    "word_choice_counts = df['word_choice'].value_counts()\n",
    "sentence_fluency_counts = df['sentence_fluency'].value_counts()\n",
    "conventions_counts = df['conventions'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "{\n",
    "    \"Organization\": organization_counts.to_dict(),\n",
    "    \"Word Choice\": word_choice_counts.to_dict(),\n",
    "    \"Sentence Fluency\": sentence_fluency_counts.to_dict(),\n",
    "    \"Conventions\": conventions_counts.to_dict()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A multitask neural network model for predicting classification scores\n",
    "    for various essay attributes such as organization, word choice, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, num_classes=7):\n",
    "        super(MultiTaskClassificationModel, self).__init__()\n",
    "        # Shared layers\n",
    "        self.fc1 = nn.Linear(input_shape, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Create separate classification heads for each feature\n",
    "        self.organization_head = nn.Linear(128, num_classes)\n",
    "        self.word_choice_head = nn.Linear(128, num_classes)\n",
    "        self.sentence_fluency_head = nn.Linear(128, num_classes)\n",
    "        self.conventions_head = nn.Linear(128, num_classes)\n",
    "\n",
    "        # Optional task uncertainty parameter\n",
    "        self.task_uncertainty = nn.Parameter(torch.tensor([0.0, 0.0]), requires_grad=True)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the shared and task-specific layers.\n",
    "        :param x: Input tensor\n",
    "        :return: Outputs for each task\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Output for each feature\n",
    "        organization_output = self.organization_head(x)\n",
    "        word_choice_output = self.word_choice_head(x)\n",
    "        sentence_fluency_output = self.sentence_fluency_head(x)\n",
    "        conventions_output = self.conventions_head(x)\n",
    "\n",
    "        return organization_output, word_choice_output, sentence_fluency_output, conventions_output\n",
    "\n",
    "    def compute_uncertainty_loss(self, loss_organization, loss_word_choice, loss_sentence_fluency, loss_conventions):\n",
    "        \"\"\"\n",
    "        Compute dynamically weighted loss using task uncertainty parameters.\n",
    "        :return: Total weighted loss\n",
    "        \"\"\"\n",
    "        organization_precision = torch.exp(-self.task_uncertainty[1])\n",
    "        word_choice_precision = torch.exp(-self.task_uncertainty[1])\n",
    "        sentence_fluency_precision = torch.exp(-self.task_uncertainty[1])\n",
    "        conventions_precision = torch.exp(-self.task_uncertainty[1])\n",
    "\n",
    "        # Weighted loss calculation\n",
    "        loss = (organization_precision * loss_organization + self.task_uncertainty[1]) + \\\n",
    "                (word_choice_precision * loss_word_choice + self.task_uncertainty[1]) + \\\n",
    "                (sentence_fluency_precision * loss_sentence_fluency + self.task_uncertainty[1]) + \\\n",
    "                (conventions_precision * loss_conventions + self.task_uncertainty[1])\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def compute_loss(self, pred_organization, pred_word_choice, pred_sentence_fluency, pred_conventions,\n",
    "                        y_organization, y_word_choice, y_sentence_fluency, y_conventions) :\n",
    "\n",
    "        \"\"\"\n",
    "        Compute total loss across all tasks.\n",
    "        :return: Combined loss\n",
    "        \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        mse_loss_organization = criterion(pred_organization, y_organization)\n",
    "        mse_loss_word_choice = criterion(pred_word_choice, y_word_choice)\n",
    "        mse_loss_sentence_fluency = criterion(pred_sentence_fluency, y_sentence_fluency)\n",
    "        mse_loss_conventions = criterion(pred_conventions, y_conventions)\n",
    "\n",
    "        total_loss = mse_loss_organization + mse_loss_word_choice + mse_loss_sentence_fluency + mse_loss_conventions\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss function that incorporates label smoothing into the standard CrossEntropyLoss.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Compute the label-smoothed cross-entropy loss.\n",
    "        :return: compute loss\n",
    "        \"\"\"\n",
    "\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        return (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_albert_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate ALBERT embeddings for a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The embedding vector from ALBERT's last hidden state.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text and send to the device (CPU/GPU)\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=256).to(device)\n",
    "    \n",
    "    # Generate embeddings without computing gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = albert_model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding from the last hidden state\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "def get_word_embedding(text, embedding_dict):\n",
    "    \"\"\"\n",
    "    Generate word embeddings for a given text using a pre-trained embedding dictionary.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        embedding_dict (dict): Pre-trained word embedding dictionary (e.g., GloVe or FastText).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The average word embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    # Split text into words and fetch embeddings for each word if available\n",
    "    words = text.lower().split()\n",
    "    vectors = [embedding_dict[word] for word in words if word in embedding_dict]\n",
    "    \n",
    "    # Compute the average embedding if vectors are found; otherwise return a zero vector\n",
    "    if vectors:\n",
    "        return torch.mean(torch.stack(vectors), dim=0).cpu().numpy()\n",
    "    return np.zeros(300)  # Default to 300 dimensions\n",
    "\n",
    "def create_attention_based_embedding(albert_emb, additional_emb):\n",
    "    \"\"\"\n",
    "    Create an attention-based fused embedding from ALBERT and additional embeddings.\n",
    "\n",
    "    Args:\n",
    "        albert_emb (torch.Tensor): ALBERT embedding vector.\n",
    "        additional_emb (torch.Tensor): Additional embedding vector (e.g., GloVe or FastText).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Fused embedding based on learned attention weights.\n",
    "    \"\"\"\n",
    "    # Ensure both embeddings have the same shape\n",
    "    if albert_emb.shape != additional_emb.shape:\n",
    "        additional_emb = torch.nn.Linear(additional_emb.shape[0], albert_emb.shape[0]).to(albert_emb.device)(additional_emb)\n",
    "    \n",
    "    # Combine embeddings into a tensor stack\n",
    "    combined_emb = torch.cat([albert_emb.unsqueeze(0), additional_emb.unsqueeze(0)], dim=0)\n",
    "    \n",
    "    # Learn attention weights dynamically\n",
    "    attention_weights = torch.nn.Parameter(torch.tensor([0.5, 0.5], device=albert_emb.device), requires_grad=True)\n",
    "    attention_scores = F.softmax(attention_weights, dim=0)\n",
    "    \n",
    "    # Compute the fused embedding as a weighted sum\n",
    "    fused_embedding = attention_scores[0] * albert_emb + attention_scores[1] * additional_emb\n",
    "    return fused_embedding\n",
    "\n",
    "\n",
    "def create_combined_embedding(text, embedding_type=None, _glove_model=None, _fasttext_model=None):\n",
    "    \"\"\"\n",
    "    Generate a combined embedding by fusing ALBERT and an additional embedding (GloVe/FastText).\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        embedding_type (str, optional): Type of additional embedding (\"glove\" or \"fasttext\"). Default is None.\n",
    "        _glove_model (dict, optional): GloVe embedding dictionary. Required if embedding_type is \"glove\".\n",
    "        _fasttext_model (dict, optional): FastText embedding dictionary. Required if embedding_type is \"fasttext\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined embedding as a numpy array and its size.\n",
    "    \"\"\"\n",
    "    # Get ALBERT embedding\n",
    "    albert_emb = get_albert_embedding(text).flatten()\n",
    "\n",
    "    # Get the additional embedding based on the specified type\n",
    "    if embedding_type == \"glove\":\n",
    "        additional_emb = get_word_embedding(text, _glove_model)\n",
    "    elif embedding_type == \"fasttext\":\n",
    "        additional_emb = get_word_embedding(text, _fasttext_model)\n",
    "    else:\n",
    "        additional_emb = np.array([])\n",
    "\n",
    "    # Convert ALBERT embedding to tensor\n",
    "    albert_emb_tensor = torch.tensor(albert_emb, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Combine ALBERT and additional embeddings, ensuring equal size\n",
    "    if additional_emb.size != 0:\n",
    "        additional_emb_tensor = torch.tensor(additional_emb, dtype=torch.float32).to(device)\n",
    "        if additional_emb_tensor.size(0) > albert_emb_tensor.size(0):\n",
    "            additional_emb_tensor = additional_emb_tensor[:albert_emb_tensor.size(0)]\n",
    "        elif additional_emb_tensor.size(0) < albert_emb_tensor.size(0):\n",
    "            padding_size = albert_emb_tensor.size(0) - additional_emb_tensor.size(0)\n",
    "            additional_emb_tensor = F.pad(additional_emb_tensor, (0, padding_size))\n",
    "        combined_emb = torch.cat([albert_emb_tensor, additional_emb_tensor], dim=0)\n",
    "    else:\n",
    "        combined_emb = albert_emb_tensor\n",
    "\n",
    "    # Return the combined embedding and its size\n",
    "    return combined_emb.cpu().numpy(), combined_emb.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(X_train_tensor, y_train_organization_tensor, \n",
    "                         y_train_word_choice_tensor, y_train_sentence_fluency_tensor, \n",
    "                         y_train_conventions_tensor, input_shape, save_dir, \n",
    "                         embedding_type=None, epochs=10, batch_size=6, learning_rate=1e-4):\n",
    "\n",
    "    model = MultiTaskClassificationModel(input_shape).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(\n",
    "            X_train_tensor,\n",
    "            y_train_organization_tensor, y_train_word_choice_tensor, \n",
    "            y_train_sentence_fluency_tensor, y_train_conventions_tensor), \n",
    "            batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "    \n",
    "    train_losses = []  # Initialize to store the loss for each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for X_batch, y_organization_batch, y_word_choice_batch, y_sentence_fluency_batch, y_conventions_batch in train_loader:\n",
    "            # Move data to device\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_organization_batch = y_organization_batch.to(device)\n",
    "            y_word_choice_batch = y_word_choice_batch.to(device)\n",
    "            y_sentence_fluency_batch = y_sentence_fluency_batch.to(device)\n",
    "            y_conventions_batch = y_conventions_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get model predictions\n",
    "            pred_organization, pred_word_choice, pred_sentence_fluency, pred_conventions = model(X_batch)\n",
    "\n",
    "            # Compute the losses\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss_organization = criterion(pred_organization, y_organization_batch.long())\n",
    "            loss_word_choice = criterion(pred_word_choice, y_word_choice_batch.long())\n",
    "            loss_sentence_fluency = criterion(pred_sentence_fluency, y_sentence_fluency_batch.long())\n",
    "            loss_conventions = criterion(pred_conventions, y_conventions_batch.long())\n",
    "\n",
    "            # Total loss\n",
    "            total_loss = loss_organization + loss_word_choice + \\\n",
    "                         loss_sentence_fluency + loss_conventions\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_epoch_loss)  # Append the average epoch loss to train_losses\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Total Epoch Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Save the trained model and embedding size\n",
    "    model_filename = f\"albert_argumentative_model_{embedding_type or 'albert'}.pth\"  # Model filename\n",
    "    embedding_size_filename = f\"albert_argumentative_embedding_size_{embedding_type or 'albert'}.npy\"  # Embedding size filename\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, model_filename))  # Save model parameters\n",
    "    np.save(os.path.join(save_dir, embedding_size_filename), input_shape)  # Save embedding size\n",
    "    \n",
    "    return os.path.join(save_dir, model_filename), train_losses  # Return model file path and training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, y_test_organization, y_test_word_choice, y_test_sentence_fluency,\n",
    "                   y_test_conventions, save_dir, model_name):\n",
    "\n",
    "    # Load the model and move it to the appropriate device\n",
    "    model = MultiTaskClassificationModel(X_test_tensor.shape[1]).to(device)  # Move model to device\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Move test tensors to the correct device\n",
    "    y_test_organization = y_test_organization.to(device)\n",
    "    y_test_word_choice = y_test_word_choice.to(device)\n",
    "    y_test_sentence_fluency = y_test_sentence_fluency.to(device)\n",
    "    y_test_conventions = y_test_conventions.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get model predictions (all outputs)\n",
    "        pred_organization, pred_word_choice, pred_sentence_fluency, pred_conventions = model(X_test_tensor.to(device))\n",
    "\n",
    "        # Kappa scores for each attribute\n",
    "        kappa_organization = cohen_kappa_score(\n",
    "            y_test_organization.cpu().numpy(),\n",
    "            np.argmax(pred_organization.cpu().numpy(), axis=1).astype(int),\n",
    "            weights='quadratic'\n",
    "        )\n",
    "        kappa_word_choice = cohen_kappa_score(\n",
    "            y_test_word_choice.cpu().numpy(),\n",
    "            np.argmax(pred_word_choice.cpu().numpy(), axis=1).astype(int),\n",
    "            weights='quadratic'\n",
    "        )\n",
    "        kappa_sentence_fluency = cohen_kappa_score(\n",
    "            y_test_sentence_fluency.cpu().numpy(),\n",
    "            np.argmax(pred_sentence_fluency.cpu().numpy(), axis=1).astype(int),\n",
    "            weights='quadratic'\n",
    "        )\n",
    "        kappa_conventions = cohen_kappa_score(\n",
    "            y_test_conventions.cpu().numpy(),\n",
    "            np.argmax(pred_conventions.cpu().numpy(), axis=1).astype(int),\n",
    "            weights='quadratic'\n",
    "        )\n",
    "\n",
    "        # Print out the evaluation results\n",
    "        print(f\"Kappa for Organization: {kappa_organization:.5f}\")\n",
    "        print(f\"Kappa for Word Choice: {kappa_word_choice:.5f}\")\n",
    "        print(f\"Kappa for Sentence Fluency: {kappa_sentence_fluency:.5f}\")\n",
    "        print(f\"Kappa for Conventions: {kappa_conventions:.5f}\")\n",
    "\n",
    "        return kappa_organization, kappa_word_choice, kappa_sentence_fluency, kappa_conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for embedding type: albert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model with the current embedding type and save it for future evaluation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining model for embedding type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m model_path, _ \u001b[38;5;241m=\u001b[39m train_and_save_model(\n\u001b[1;32m     44\u001b[0m     X_train_tensor,\n\u001b[1;32m     45\u001b[0m     y_train_organization_tensor,\n\u001b[1;32m     46\u001b[0m     y_train_word_choice_tensor,\n\u001b[1;32m     47\u001b[0m     y_train_sentence_fluency_tensor,\n\u001b[1;32m     48\u001b[0m     y_train_conventions_tensor,\n\u001b[1;32m     49\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39mX_train_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# Pass as keyword argument\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39mSAVE_DIR,\n\u001b[1;32m     51\u001b[0m     embedding_type\u001b[38;5;241m=\u001b[39membedding_type,\n\u001b[1;32m     52\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     53\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m     54\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Evaluate the trained model and collect metrics, particularly Kappa scores for each attribute\u001b[39;00m\n\u001b[1;32m     58\u001b[0m kappa_organization, kappa_word_choice, kappa_sentence_fluency, kappa_conventions \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[1;32m     59\u001b[0m     model_path,\n\u001b[1;32m     60\u001b[0m     y_test_organization_tensor, y_test_word_choice_tensor, \n\u001b[1;32m     61\u001b[0m     y_test_sentence_fluency_tensor, y_test_conventions_tensor, SAVE_DIR, model_name\n\u001b[1;32m     62\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mtrain_and_save_model\u001b[0;34m(X_train_tensor, y_train_organization_tensor, y_train_word_choice_tensor, y_train_sentence_fluency_tensor, y_train_conventions_tensor, input_shape, save_dir, embedding_type, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Compute the losses\u001b[39;00m\n\u001b[1;32m     36\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 37\u001b[0m loss_organization \u001b[38;5;241m=\u001b[39m criterion(pred_organization, y_organization_batch\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     38\u001b[0m loss_word_choice \u001b[38;5;241m=\u001b[39m criterion(pred_word_choice, y_word_choice_batch\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     39\u001b[0m loss_sentence_fluency \u001b[38;5;241m=\u001b[39m criterion(pred_sentence_fluency, y_sentence_fluency_batch\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/.conda/envs/ai_aes/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ai_aes/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ai_aes/lib/python3.11/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1295\u001b[0m         target,\n\u001b[1;32m   1296\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1297\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[1;32m   1298\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1299\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[1;32m   1300\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/ai_aes/lib/python3.11/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3481\u001b[0m     target,\n\u001b[1;32m   3482\u001b[0m     weight,\n\u001b[1;32m   3483\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[1;32m   3484\u001b[0m     ignore_index,\n\u001b[1;32m   3485\u001b[0m     label_smoothing,\n\u001b[1;32m   3486\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Main training and evaluation loop\n",
    "embedding_types = [None, \"glove\", \"fasttext\"]\n",
    "all_train_losses = []  # To store training losses for each embedding type\n",
    "embedding_labels = [embedding_type or 'albert' for embedding_type in embedding_types]\n",
    "all_kappa_scores = []  # Initialize list to store Kappa scores for each model\n",
    "\n",
    "for embedding_type in embedding_types:\n",
    "    # Set model_name based on the embedding type for evaluation\n",
    "    model_name = embedding_type or 'albert'\n",
    "    \n",
    "    embeddings_and_sizes = df['essay'].apply(lambda x: create_combined_embedding(x, embedding_type, glove_model, fasttext_model))\n",
    "    df['embeddings'], embedding_sizes = zip(*embeddings_and_sizes)\n",
    "\n",
    "    embedding_sizes = np.array(embedding_sizes)\n",
    "\n",
    "    X_train, X_test, y_train_organization, y_test_organization, \\\n",
    "    y_train_word_choice, y_test_word_choice, y_train_sentence_fluency, y_test_sentence_fluency, \\\n",
    "    y_train_conventions, y_test_conventions = train_test_split(\n",
    "        np.stack(df['embeddings'].values),\n",
    "        df['organization'].values,\n",
    "        df['word_choice'].values,\n",
    "        df['sentence_fluency'].values,\n",
    "        df['conventions'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert each subset to PyTorch tensors for compatibility with the training process\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_organization_tensor = torch.tensor(y_train_organization, dtype=torch.float32)\n",
    "    y_train_word_choice_tensor = torch.tensor(y_train_word_choice, dtype=torch.float32)\n",
    "    y_train_sentence_fluency_tensor = torch.tensor(y_train_sentence_fluency, dtype=torch.float32)\n",
    "    y_train_conventions_tensor = torch.tensor(y_train_conventions, dtype=torch.float32)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_organization_tensor = torch.tensor(y_test_organization, dtype=torch.float32)\n",
    "    y_test_word_choice_tensor = torch.tensor(y_test_word_choice, dtype=torch.float32)\n",
    "    y_test_sentence_fluency_tensor = torch.tensor(y_test_sentence_fluency, dtype=torch.float32)\n",
    "    y_test_conventions_tensor = torch.tensor(y_test_conventions, dtype=torch.float32)\n",
    "\n",
    "    # Train the model with the current embedding type and save it for future evaluation\n",
    "    print(f\"\\nTraining model for embedding type: {model_name}\")\n",
    "    model_path, _ = train_and_save_model(\n",
    "        X_train_tensor,\n",
    "        y_train_organization_tensor,\n",
    "        y_train_word_choice_tensor,\n",
    "        y_train_sentence_fluency_tensor,\n",
    "        y_train_conventions_tensor,\n",
    "        input_shape=X_train_tensor.shape[1],  # Pass as keyword argument\n",
    "        save_dir=SAVE_DIR,\n",
    "        embedding_type=embedding_type,\n",
    "        epochs=10,\n",
    "        batch_size=6,\n",
    "        learning_rate=1e-3\n",
    "    )\n",
    "\n",
    "    # Evaluate the trained model and collect metrics, particularly Kappa scores for each attribute\n",
    "    kappa_organization, kappa_word_choice, kappa_sentence_fluency, kappa_conventions = evaluate_model(\n",
    "        model_path,\n",
    "        y_test_organization_tensor, y_test_word_choice_tensor, \n",
    "        y_test_sentence_fluency_tensor, y_test_conventions_tensor, SAVE_DIR, model_name\n",
    "    )\n",
    "\n",
    "    # Append Kappa scores for each attribute to all_kappa_scores to plot later\n",
    "    all_kappa_scores.append([kappa_organization, kappa_word_choice, kappa_sentence_fluency, \\\n",
    "        kappa_conventions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "    In “Let there be dark,” Paul Bogard talks about the importance of darkness.\n",
    "Darkness is essential to humans. Bogard states, “Our bodies need darkness to produce the hormone melatonin, which keeps certain cancers from developing, and our bodies need darkness for sleep, sleep. Sleep disorders have been linked to diabetes, obesity, cardiovascular disease and depression and recent research suggests are main cause of “short sleep” is “long light.” Whether we work at night or simply take our tablets, notebooks and smartphones to bed, there isn’t a place for this much artificial light in our lives.” (Bogard 2). Here, Bogard talks about the importance of darkness to humans. Humans need darkness to sleep in order to be healthy.\n",
    "Animals also need darkness. Bogard states, “The rest of the world depends on darkness as well, including nocturnal and crepuscular species of birds, insects, mammals, fish and reptiles. Some examples are well known—the 400 species of birds that migrate at night in North America, the sea turtles that come ashore to lay their eggs—and some are not, such as the bats that save American farmers billions in pest control and the moths that pollinate 80% of the world’s flora. Ecological light pollution is like the bulldozer of the night, wrecking habitat and disrupting ecosystems several billion years in the making. Simply put, without darkness, Earth’s ecology would collapse...” (Bogard 2). Here Bogard explains that animals, too, need darkness to survive.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testContent(content, embedding_type=None, SAVE_DIR=None, glove_model=None, fasttext_model=None):\n",
    "    \"\"\"\n",
    "    Test the model on a single piece of content by generating predictions for multiple attributes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Predictions for the following attributes:\n",
    "    \"\"\"\n",
    "    # Generate a combined embedding (ALBERT + optional additional embedding)\n",
    "    embedding, actual_embedding_size = create_combined_embedding(\n",
    "        content,\n",
    "        embedding_type=embedding_type,\n",
    "        _glove_model=glove_model if embedding_type == \"glove\" else None,\n",
    "        _fasttext_model=fasttext_model if embedding_type == \"fasttext\" else None\n",
    "    )\n",
    "\n",
    "    # Convert the embedding to a PyTorch tensor and add a batch dimension\n",
    "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "    # File paths for the saved model and embedding size\n",
    "    embedding_size_filename = f\"albert6_embedding_size_{embedding_type or 'albert'}.npy\"\n",
    "    model_filename = f\"albert6_model_{embedding_type or 'albert'}.pth\"\n",
    "\n",
    "    # Load the expected embedding size and ensure it matches the current embedding\n",
    "    embedding_size_path = os.path.join(SAVE_DIR, embedding_size_filename)\n",
    "    expected_embedding_size = int(np.load(embedding_size_path))\n",
    "\n",
    "    # Initialize the model and load its weights\n",
    "    model = MultiTaskClassificationModel(expected_embedding_size).to(device)\n",
    "    model_path = os.path.join(SAVE_DIR, model_filename)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Adjust the embedding size if it differs from the expected size\n",
    "    embedding_resized = embedding_tensor[:, :expected_embedding_size]\n",
    "\n",
    "    # Generate predictions for all attributes\n",
    "    with torch.no_grad():\n",
    "        pred_organization, pred_word_choice, pred_sentence_fluency, pred_conventions = model(embedding_resized)\n",
    "\n",
    "        # Extract the predicted class (highest probability) for each attribute\n",
    "        organization_score = pred_organization.cpu().item()\n",
    "        word_choice_score = pred_word_choice.cpu().item()\n",
    "        sentence_fluency_score = pred_sentence_fluency.cpu().item()\n",
    "        conventions_score = pred_conventions.cpu().item()\n",
    "\n",
    "    # Return predictions as a tuple\n",
    "    return organization_score, word_choice_score, sentence_fluency_score, conventions_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Essay Scores for ALBERT:\n",
      "  Organization Score: 5.00\n",
      "  Word Choice Score: 5.00\n",
      "  Sentence Fluency Score: 5.00\n",
      "  Conventions Score: 5.00\n",
      "Sample Essay Scores for ALBERT + GloVe:\n",
      "  Organization Score: 5.00\n",
      "  Word Choice Score: 6.00\n",
      "  Sentence Fluency Score: 5.00\n",
      "  Conventions Score: 5.00\n",
      "Sample Essay Scores for ALBERT + FastText:\n",
      "  Organization Score: 5.00\n",
      "  Word Choice Score: 6.00\n",
      "  Sentence Fluency Score: 5.00\n",
      "  Conventions Score: 5.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm-salsabila.pranida-52436/ipykernel_2639920/1667596934.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate a sample essay using different embedding types (ALBERT, ALBERT + GloVe, ALBERT + FastText)\n",
    "and store the results in a dictionary. The evaluation generates scores for the following attributes:\n",
    "Organization, Word Choice, Sentence Fluency, and Conventions.\n",
    "\n",
    "Returns:\n",
    "    dict: A dictionary with embedding types as keys and scores for each attribute as values.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize an empty dictionary to store results for each embedding type\n",
    "results = {}\n",
    "\n",
    "# Define the embedding types to evaluate\n",
    "embedding_types = [None, \"glove\", \"fasttext\"]\n",
    "\n",
    "# Loop through each embedding type\n",
    "for embedding_type in embedding_types:\n",
    "    # Set a readable name for the embedding type\n",
    "    if embedding_type is None:\n",
    "        embedding_type_name = \"ALBERT\"\n",
    "    elif embedding_type == \"glove\":\n",
    "        embedding_type_name = \"ALBERT + GloVe\"\n",
    "    elif embedding_type == \"fasttext\":\n",
    "        embedding_type_name = \"ALBERT + FastText\"\n",
    "\n",
    "    # Generate predictions for the sample content using the current embedding type\n",
    "    organization_score, word_choice_score, sentence_fluency_score, conventions_score = testContent(\n",
    "        content=content,                # Content\n",
    "        embedding_type=embedding_type,  # Current embedding type\n",
    "        SAVE_DIR=SAVE_DIR,              # Directory containing model files\n",
    "        glove_model=glove_model,        # GloVe embeddings (if applicable)\n",
    "        fasttext_model=fasttext_model   # FastText embeddings (if applicable)\n",
    "    )\n",
    "\n",
    "    # # Log the embedding type and the returned scores\n",
    "    # print(f\"Embedding Type: {embedding_type_name}\")\n",
    "    # print(f\"Returned Values: {organization_score}, {word_choice_score}, {sentence_fluency_score}, {conventions_score}\")\n",
    "\n",
    "    # Store the scores in the results dictionary\n",
    "    try:\n",
    "        results[embedding_type_name] = {\n",
    "            \"Organization Score\": float(organization_score),\n",
    "            \"Word Choice Score\": float(word_choice_score),\n",
    "            \"Sentence Fluency Score\": float(sentence_fluency_score),\n",
    "            \"Conventions Score\": float(conventions_score)\n",
    "        }\n",
    "    except ValueError:\n",
    "        # Handle potential conversion issues\n",
    "        print(f\"Error: Unable to convert one or more values to float for embedding type: {embedding_type_name}\")\n",
    "        print(f\"Values: {organization_score}, {word_choice_score}, {sentence_fluency_score}, {conventions_score}\")\n",
    "\n",
    "# Display the results for each embedding type\n",
    "for embedding_name, result in results.items():\n",
    "    print(f\"Sample Essay Scores for {embedding_name}:\")\n",
    "    print(f\"  Organization Score: {result['Organization Score']:.2f}\")\n",
    "    print(f\"  Word Choice Score: {result['Word Choice Score']:.2f}\")\n",
    "    print(f\"  Sentence Fluency Score: {result['Sentence Fluency Score']:.2f}\")\n",
    "    print(f\"  Conventions Score: {result['Conventions Score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_aes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
